# LLM-non-determinism-evaluator
Este Ã© o projeto da disciplina de tÃ³picos avanÃ§ados em engenharia de software. Nele iremos iterar sobre o trabalho [An Empirical Study of the Non-Determinism of ChatGPT in Code Generation](https://dl.acm.org/doi/full/10.1145/36970100). A equipe pretende aplicar as seguintes anÃ¡lises:
- Aplicar e avaliar o nÃ£o determinismo em diferentes modelos de linguagem;
- Implementar loops para garantir corretude do cÃ³digo gerado.
- Analisar a intersecÃ§Ã£o entre corretude semÃ¢ntica e sintÃ¡tica do cÃ³digo gerado.

## Preparing the model

```sh
curl -fsSL https://ollama.com/install.sh | sh
ollama pull llama3.2
```

## Estrutura do Projeto

```
LLM-non-determinism-evaluator/
â”œâ”€â”€ datasets/                    # Dataset e resultados dos experimentos
â”‚   â”œâ”€â”€ human_eval.json         # Dataset HumanEval
â”‚   â””â”€â”€ exp_1_*.json           # Resultados dos experimentos
â”œâ”€â”€ reports/                    # RelatÃ³rios e visualizaÃ§Ãµes
â”‚   â”œâ”€â”€ analysis/              # RelatÃ³rios de anÃ¡lise JSON
â”‚   â””â”€â”€ visualizations/        # GrÃ¡ficos e matrizes PNG
â”œâ”€â”€ utils/                     # MÃ³dulos auxiliares
â”‚   â”œâ”€â”€ analysis.py           # AnÃ¡lise sintÃ¡tica e semÃ¢ntica
â”‚   â”œâ”€â”€ dataset_handler.py    # ManipulaÃ§Ã£o do dataset
â”‚   â”œâ”€â”€ llm_handler.py        # Interface com o modelo LLM
â”‚   â””â”€â”€ module_loader.py      # Carregamento de mÃ³dulos
â”œâ”€â”€ current_task/              # DiretÃ³rio temporÃ¡rio para tarefas
â”œâ”€â”€ experiment_1.py           # Experimento principal
â”œâ”€â”€ analyze_results.py        # Script de anÃ¡lise
â”œâ”€â”€ visualize_results.py      # Script de visualizaÃ§Ã£o
â”œâ”€â”€ cleanup.py               # Script de limpeza e organizaÃ§Ã£o
â””â”€â”€ requirements.txt          # DependÃªncias Python
```

## Experimentos 

1. **Experimento 1**: Coletar 5 soluÃ§Ãµes por task geradas pelo llama3.2 e comparar sintaxe (conteudo da string), semÃ¢ntica (resultado do teste) e estrutura (AST)
2. **Experimento 2**: IteraÃ§Ã£o com feedback atÃ© obter soluÃ§Ã£o correta (mÃ¡ximo 5 tentativas)
3. **Experimento 3**: VariaÃ§Ã£o de parÃ¢metros do modelo - 5 configuraÃ§Ãµes diferentes por task:
   - Conservative (temp=0.1, top_p=0.8, top_k=20)
   - Balanced (temp=0.7, top_p=0.9, top_k=40) 
   - Creative (temp=1.2, top_p=0.95, top_k=80)
   - High_Randomness (temp=1.5, top_p=1.0, top_k=100)
   - Focused (temp=0.3, top_p=0.7, top_k=10)

## Como Usar

### 1. Executar Experimento 1
```bash
python3 experiment_1.py
```
- Gera 5 soluÃ§Ãµes por tarefa
- Executa testes automÃ¡ticos
- Salva resultados em `datasets/exp_1_*.json`
- Gera anÃ¡lise automÃ¡tica em `reports/analysis/analysis_report_*.json`

### 2. Executar Experimento 2
```bash
python3 experiment_2.py
```
- Itera atÃ© obter soluÃ§Ã£o correta (mÃ¡x 5 tentativas)
- Usa feedback dos erros para melhorar prÃ³xima tentativa
- Salva resultados em `datasets/exp_2_*.json`

### 3. Executar Experimento 3
```bash
python3 experiment_3.py
```
- Testa 5 configuraÃ§Ãµes diferentes de parÃ¢metros por task
- Avalia impacto da temperature, top_p, top_k e num_predict
- Salva resultados em `datasets/exp_3_*.json`
- Gera anÃ¡lise comparativa por configuraÃ§Ã£o

### 2. Analisar Resultados
```bash
# AnÃ¡lise geral
python3 analyze_results.py datasets/exp_1_250728-202215.json

# AnÃ¡lise de tarefa especÃ­fica
python3 analyze_results.py datasets/exp_1_250728-202215.json HumanEval_0
```

### 3. Criar VisualizaÃ§Ãµes
```bash
python3 visualize_results.py datasets/exp_1_250728-202215.json
```
- Gera grÃ¡ficos em `reports/visualizations/visualization_*.png`
- Gera matrizes de similaridade em `reports/visualizations/comparison_matrix_*.png`
```

## MÃ©tricas Implementadas

### AnÃ¡lise SintÃ¡tica
- **Similaridade de strings** (difflib)
- **Similaridade de tokens** (identificadores, funÃ§Ãµes)
- **Hash MD5** para match exato
- **NormalizaÃ§Ã£o** de cÃ³digo

### AnÃ¡lise Estrutural (AST)
- **Estrutura de funÃ§Ãµes** (nome, argumentos, decoradores)
- **Controle de fluxo** (if, for, while)
- **Imports e expressÃµes**
- **Similaridade estrutural**

### AnÃ¡lise SemÃ¢ntica
- **Taxa de sucesso** nos testes
- **ConsistÃªncia semÃ¢ntica** (todas passam ou todas falham)
- **Score de nÃ£o-determinismo** (1 - similaridade mÃ©dia)

## Exemplo de Resultados

Para HumanEval_0 (has_close_elements):
```
ğŸ“Š AnÃ¡lise para HumanEval_0:
   â€¢ Taxa de sucesso: 80.00%
   â€¢ Similaridade sintÃ¡tica mÃ©dia: 52.97%
   â€¢ Similaridade AST mÃ©dia: 87.54%
   â€¢ Score de nÃ£o-determinismo: 47.03%
   â€¢ ConsistÃªncia semÃ¢ntica: âŒ
```

## OrganizaÃ§Ã£o de Arquivos

### Resultados dos Experimentos
- **LocalizaÃ§Ã£o**: `datasets/exp_1_*.json`
- **ConteÃºdo**: CÃ³digo gerado, resultados de teste, anÃ¡lises
- **Formato**: JSON com timestamp

### RelatÃ³rios de AnÃ¡lise
- **LocalizaÃ§Ã£o**: `reports/analysis/analysis_report_*.json`
- **ConteÃºdo**: MÃ©tricas de similaridade, estatÃ­sticas agregadas
- **Formato**: JSON estruturado

### VisualizaÃ§Ãµes
- **LocalizaÃ§Ã£o**: `reports/visualizations/`
- **Tipos**: GrÃ¡ficos de barras, matrizes de similaridade
- **Formato**: PNG de alta resoluÃ§Ã£o

## PrÃ³ximos Passos

1. **Expandir para 164 tarefas** (dataset completo)
2. **Testar outros modelos** (codellama, mistral)
3. **Adicionar mais mÃ©tricas** (complexidade ciclomÃ¡tica)
4. **Implementar anÃ¡lise temporal**
5. **Criar dashboard interativo**

## ManutenÃ§Ã£o

O script `cleanup.py` ajuda a manter a organizaÃ§Ã£o:
- **Limpa arquivos temporÃ¡rios** do `current_task/`
- **MantÃ©m apenas os Ãºltimos 5 relatÃ³rios** de anÃ¡lise
- **MantÃ©m apenas as Ãºltimas 10 visualizaÃ§Ãµes**
- **Cria backups** dos arquivos importantes